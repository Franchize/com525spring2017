---
layout: post
title: "Basic Methods and Intro to User Testing"
comments: false
description: "Here's what we covered in the first week of class"
---
# Administrivia

- Updated site with Xi's office number and hours (Mon 10-11am)
- Blackboard should work now

# Review

What did we cover last time?

# New Stuff

## Three main types of UX evaluations

- ask
	- interviews
	- surveys
	- focus groups
- observe
	- ethnographic
	- testing
	- analytics
- inspect
	- guidelines (e.g., heuristics)
	- walkthroughs
	- comparative and competitive analyses

Techniques can be mixed - e.g. observe and ask

## When should I use which method?

Most projects use all of the types of methods at least some of the time.

| Method Type        | Good Fit          | Poor Fit |
| ------------- |-------------| -----|
| Ask | - observe won't work<br>- want to get at values and motivations<br>- you need big N (surveys)|- you need people to remember or explain how to do something |
| Inspect | - prototype<br>- users are too expensive<br>- need to know about competitors | - early in design (before you have a thing)
| Observe|- asking won't work<br>- want to get at process<br>- you need big N (analytics) |- users are inaccessible<br>- privacy paramount<br>- rare activity | 

*Example product cycle*

Another way of thinking about these choices:
![Landscape of UX Research Methods](https://s3.amazonaws.com/media.nngroup.com/media/editor/2014/10/10/user-research-methods.png)
* Source: [Nielsen Norman Group](https://www.nngroup.com/articles/which-ux-research-methods/)

## User Testing

Basics: watch a user type to accomplish task(s) using a product.

1. Find some users
2. Ask them to do something
3. Observe/watch them try* to do the task
4. Debrief (ask them about what you saw)
5. Report what you learned

### What does user testing get you?

- what works and what doesn't
- why things work and don't
- user needs you missed or misunderstood

### Who should I recruit?

- target users
- not current users

### What tasks should I choose?

- something *most* users need to do
- things that are *hard* for users to do but have value
- something that has a clear, verifiable end
- has an expected path (there's a right way to do it)
- start "easy"

### What happens during the test?

- "think aloud"
	- looking, reading
	- guessing how the system will work (expectations)
	- interpreting options and feedback
	- explaining behaviors and decisions
	- feelings
	- ok to model
- don't lead the user (Amazon shopping example)
- have the user tell you when you're done (e.g., "I'm done.")

### Iterate!

### How do I debrief?

- **Take notes** during the test
- Ask about things you noticed - e.g. places the user got stuck, when they went backwards, showed signs of frustration
- Value, usability, desirability, adoptability

### After the test

- **take notes** you won't remember, don't try
- critical incidents
	- mismatch with user expectations
	- invalid assumptions
	- too little or too much flexibility
- success/failures and their severity

# Exercise

Before we go to the physical bus stops...design a user test about the CTA web/mobile sites:

1. how to get from MTCC to United Center for Twenty One Pilots concert Jan 28 at 5:30 pm
2. how much will the trip cost

# Acknowledgements

More thanks to [Mark Newman](https://www.si.umich.edu/people/mark-newman) for lecture material.

